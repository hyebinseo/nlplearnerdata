{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bf0b0c",
   "metadata": {},
   "source": [
    "### Save path, text, their split version, file names (text ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daefa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8d0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH of folder containing learner texts in txt\n",
    "folder_path = r\"C:\\Users\\seohy\\nlplearnerdata\\raw_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d5b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/braj29/Text-to-Conll/blob/main/texts_to_conll.py\n",
    "# Copy-pasted and adjusted the code for converting txt to conllu\n",
    "# Get paths of each learner text from folder_path\n",
    "def get_paths(path):\n",
    "    paths_ = glob.glob(path + \"/*.txt\")\n",
    "    return paths_\n",
    "\n",
    "def load_text(txt_path):\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        content = infile.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f4bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_paths = get_paths(folder_path) # PATHs of .txt files\n",
    "texts = [] # raw text\n",
    "sentences = [] # raw text into sentences\n",
    "names = [] # names of files (text id)\n",
    "\n",
    "for txt_path in txt_paths:\n",
    "    text = load_text(txt_path)\n",
    "\n",
    "    texts.append(text)\n",
    "    sentences.append(sent_tokenize(text))\n",
    "    \n",
    "    # Source: https://medium.com/@emoome82/extracting-file-names-without-extensions-in-python-caabe8532f92\n",
    "    name, extension = os.path.splitext(os.path.basename(txt_path))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f427db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\seohy\\\\nlplearnerdata\\\\raw_text\\\\1_A20210.txt', 'C:\\\\Users\\\\seohy\\\\nlplearnerdata\\\\raw_text\\\\1_A20503.txt', 'C:\\\\Users\\\\seohy\\\\nlplearnerdata\\\\raw_text\\\\1_A30406.txt']\n",
      "[\"I will introduce my favorite things. First, I love cats. Though I don't have a cat, they looks so cute. Next, I love green. Green is color of nature. I think green is a very comfortable color. Last, I like music. Music makes me satisfied. Pop songs are exciting, and Kpops are exciting, too.\", \"I will introduce my favorite k-pop group. They are boynextdoor. Boynextdoor is a team that is included six members. Every members are all rounder. Especially, they are good at singing. They use only hand mic every time. They always do their best and that's the reason why i like them a lot.\", 'I like listen to music because it makes me comfortable. and I love puppy. when i see the puppy, I feel happy. Lastly I like band LUCY\". When I listening the LUCY\\'s music,I feel happy. They give the 위로 to many people.\"']\n",
      "[['I will introduce my favorite things.', 'First, I love cats.', \"Though I don't have a cat, they looks so cute.\", 'Next, I love green.', 'Green is color of nature.', 'I think green is a very comfortable color.', 'Last, I like music.', 'Music makes me satisfied.', 'Pop songs are exciting, and Kpops are exciting, too.'], ['I will introduce my favorite k-pop group.', 'They are boynextdoor.', 'Boynextdoor is a team that is included six members.', 'Every members are all rounder.', 'Especially, they are good at singing.', 'They use only hand mic every time.', \"They always do their best and that's the reason why i like them a lot.\"], ['I like listen to music because it makes me comfortable.', 'and I love puppy.', 'when i see the puppy, I feel happy.', 'Lastly I like band LUCY\".', \"When I listening the LUCY's music,I feel happy.\", 'They give the 위로 to many people.\"']]\n",
      "['1_A20210', '1_A20503', '1_A30406']\n"
     ]
    }
   ],
   "source": [
    "# Verify path, sentences, and names\n",
    "print(txt_paths[:3])\n",
    "print(texts[:3])\n",
    "print(sentences[:3])\n",
    "print(names[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d697dcd",
   "metadata": {},
   "source": [
    "### Language-tool-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ee5686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Common Files\\Oracle\\Java\\javapath\\java.EXE\n"
     ]
    }
   ],
   "source": [
    "# Set Java PATH for current ipynb session for language_tool_python local server\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-25\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
    "\n",
    "# The following code should return the path to the java.exe file\n",
    "import shutil\n",
    "print(shutil.which(\"java\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2b9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import language_tool_python\n",
    "from language_tool_python.utils import classify_matches\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd2f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up local server \n",
    "tool = language_tool_python.LanguageTool(\"en-US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the text on the model\n",
    "for path, text, name in zip(txt_paths, texts, names):\n",
    "    match = tool.check(text)\n",
    "\n",
    "    # Convert and output results into dataframe and excel\n",
    "    df_language_tool_python = pd.DataFrame([m.__dict__ for m in match])\n",
    "    df_language_tool_python.to_excel(f\"C:\\\\Users\\\\seohy\\\\nlplearnerdata\\\\references\\\\learner_errors\\\\ref_language_tool_python\\\\{name}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52341649",
   "metadata": {},
   "source": [
    "### gector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e4fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seohy\\anaconda3\\envs\\nlplearnerdata-detect-errors\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from gector.modeling import GECToR\n",
    "from gector.predict import predict, load_verb_dict\n",
    "import torch\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0bd6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display width for the columns of df\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b94014",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gotutiyan/gector-xlnet-large-cased-5k\"\n",
    "model = GECToR.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "encode, decode = load_verb_dict(r\"C:\\Users\\seohy\\nlplearnerdata\\references\\learner_errors\\data\\verb-from-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0499fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteratoin 0. the number of to_be_processed: 9\n",
      "Iteratoin 1. the number of to_be_processed: 2\n",
      "Iteratoin 0. the number of to_be_processed: 7\n",
      "Iteratoin 1. the number of to_be_processed: 6\n",
      "Iteratoin 2. the number of to_be_processed: 3\n",
      "Iteratoin 3. the number of to_be_processed: 1\n",
      "Iteratoin 0. the number of to_be_processed: 6\n",
      "Iteratoin 1. the number of to_be_processed: 5\n",
      "Iteratoin 2. the number of to_be_processed: 1\n",
      "Iteratoin 0. the number of to_be_processed: 8\n",
      "Iteratoin 1. the number of to_be_processed: 1\n",
      "Iteratoin 0. the number of to_be_processed: 3\n",
      "Iteratoin 1. the number of to_be_processed: 3\n",
      "Iteratoin 0. the number of to_be_processed: 8\n",
      "Iteratoin 1. the number of to_be_processed: 6\n",
      "Iteratoin 2. the number of to_be_processed: 3\n",
      "Iteratoin 0. the number of to_be_processed: 9\n",
      "Iteratoin 1. the number of to_be_processed: 4\n",
      "Iteratoin 0. the number of to_be_processed: 6\n",
      "Iteratoin 1. the number of to_be_processed: 4\n",
      "Iteratoin 2. the number of to_be_processed: 3\n",
      "Iteratoin 3. the number of to_be_processed: 1\n",
      "Iteratoin 4. the number of to_be_processed: 1\n",
      "Iteratoin 0. the number of to_be_processed: 4\n",
      "Iteratoin 1. the number of to_be_processed: 3\n",
      "Iteratoin 2. the number of to_be_processed: 2\n",
      "Iteratoin 3. the number of to_be_processed: 2\n",
      "Iteratoin 4. the number of to_be_processed: 2\n",
      "Iteratoin 0. the number of to_be_processed: 8\n",
      "Iteratoin 1. the number of to_be_processed: 5\n",
      "Iteratoin 2. the number of to_be_processed: 2\n"
     ]
    }
   ],
   "source": [
    "for srcs, name in zip(sentences, names):\n",
    "    # The corrected sentence with their tags\n",
    "    corrected = None\n",
    "\n",
    "    try:\n",
    "        # inference in no_grad mode\n",
    "        with torch.no_grad():\n",
    "            corrected = predict(\n",
    "                model, tokenizer, srcs,\n",
    "                encode, decode,\n",
    "                keep_confidence=0.0,\n",
    "                min_error_prob=0.0,\n",
    "                n_iteration=5,\n",
    "                batch_size=2,\n",
    "            )\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error during inference:\", e)\n",
    "        # you could try fallback: smaller batch size or CPU mode here\n",
    "        # e.g. model.cpu(), batch_size=1, etc.\n",
    "\n",
    "    gector_df = pd.DataFrame({\n",
    "        \"Gector_correction\": corrected\n",
    "    })\n",
    "\n",
    "    gector_df.to_excel(f\"C:\\\\Users\\\\seohy\\\\nlplearnerdata\\\\references\\\\learner_errors\\\\ref_gector\\\\{name}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ef291",
   "metadata": {},
   "source": [
    "### Gramformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b5e5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gramformer import Gramformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "642331aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seohy\\anaconda3\\envs\\nlplearnerdata-detect-errors\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\seohy\\anaconda3\\envs\\nlplearnerdata-detect-errors\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n"
     ]
    }
   ],
   "source": [
    "gf_1 = Gramformer(models = 1, use_gpu = False) # 1 = corrector, 2 = detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5721495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for srcs, name in zip(sentences, names):\n",
    "    data = []\n",
    "\n",
    "    for sentence in srcs:    \n",
    "        corrected = gf_1.correct(sentence, max_candidates=1)\n",
    "        for c in corrected:\n",
    "            data.append((c))\n",
    "\n",
    "    df_gramformer = pd.DataFrame(data, columns=[\"Gramformer_correction\"])\n",
    "    df_gramformer.to_csv(f\"C:\\\\Users\\\\seohy\\\\nlplearnerdata\\\\references\\\\learner_errors\\\\ref_gramformer\\\\{name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd2690",
   "metadata": {},
   "source": [
    "### Combining the three dfs into one df and save as csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e761d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    # Read in\n",
    "    df_gector = pd.read_excel(f\"ref_gector/{name}.xlsx\", index_col = 0)\n",
    "    df_gector.loc[-1] = df_gector.columns \n",
    "    df_gector.index = df_gector.index + 1\n",
    "    df_gector = df_gector.sort_index()\n",
    "    df_gector.columns = [\"message\"]\n",
    "\n",
    "    df_gramformer = pd.read_csv(f\"ref_gramformer/{name}.csv\", index_col = 0)\n",
    "    df_gramformer.loc[-1] = df_gramformer.columns \n",
    "    df_gramformer.index = df_gramformer.index + 1\n",
    "    df_gramformer = df_gramformer.sort_index()\n",
    "    df_gramformer.columns = [\"replacements\"]\n",
    "    \n",
    "    df_language_tool_python = pd.read_excel(f\"ref_language_tool_python/{name}.xlsx\", index_col = 0)\n",
    "\n",
    "    df_gector_gramformer = pd.concat([df_gector, df_gramformer], axis = 1)\n",
    "\n",
    "    # Combine all dataframes into one dataframe\n",
    "    df_total = pd.concat([df_language_tool_python, df_gector_gramformer.reindex(columns = df_language_tool_python.columns)], axis = 0, ignore_index = True)\n",
    "\n",
    "    # Output combined df into out_total folder\n",
    "    df_total.to_csv(f\"ref_results/{name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlplearnerdata-detect-errors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
