{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bf0b0c",
   "metadata": {},
   "source": [
    "### Save path, text, their split version, file names (text ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daefa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH of folder containing learner texts in txt\n",
    "folder_path = r\"C:\\Users\\seohy\\references\\raw_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/braj29/Text-to-Conll/blob/main/texts_to_conll.py\n",
    "# Copy-pasted and adjusted the code for converting txt to conllu\n",
    "# Get paths of each learner text from folder_path\n",
    "def get_paths(path):\n",
    "    paths_ = glob.glob(path + \"/*.txt\")\n",
    "    return paths_\n",
    "\n",
    "def load_text(txt_path):\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        content = infile.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_paths = get_paths(folder_path) # PATHs of .txt files\n",
    "texts = [] # raw text\n",
    "sentences = [] # raw text into sentences\n",
    "names = [] # names of files (text id)\n",
    "\n",
    "for txt_path in txt_paths:\n",
    "    text = load_text(txt_path)\n",
    "\n",
    "    texts.append(text)\n",
    "    sentences.append(sent_tokenize(text))\n",
    "    \n",
    "    # Source: https://medium.com/@emoome82/extracting-file-names-without-extensions-in-python-caabe8532f92\n",
    "    name, extension = os.path.splitext(os.path.basename(txt_path))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify path, sentences, and names\n",
    "print(txt_paths[:3])\n",
    "print(texts[:3])\n",
    "print(sentences[:3])\n",
    "print(names[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d697dcd",
   "metadata": {},
   "source": [
    "### Language-tool-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Java PATH for current ipynb session for language_tool_python local server\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-25\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
    "\n",
    "# The following code should return the path to the java.exe file\n",
    "import shutil\n",
    "print(shutil.which(\"java\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import language_tool_python\n",
    "from language_tool_python.utils import classify_matches\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up local server \n",
    "tool = language_tool_python.LanguageTool(\"en-US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the text on the model\n",
    "for path, text, name in zip(txt_paths, texts, names):\n",
    "    match = tool.check(text)\n",
    "\n",
    "    # Convert and output results into dataframe and excel\n",
    "    df_language_tool_python = pd.DataFrame([m.__dict__ for m in match])\n",
    "    df_language_tool_python.to_excel(f\"ref_language_tool_python/{name}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52341649",
   "metadata": {},
   "source": [
    "### gector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from gector.modeling import GECToR\n",
    "from gector.predict import predict, load_verb_dict\n",
    "import torch\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display width for the columns of df\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b94014",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gotutiyan/gector-xlnet-large-cased-5k\"\n",
    "model = GECToR.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "encode, decode = load_verb_dict(r\"C:\\Users\\seohy\\references\\learner_errors\\data\\verb-from-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0499fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for srcs, name in zip(sentences, names):\n",
    "    # The corrected sentence with their tags\n",
    "    corrected = None\n",
    "\n",
    "    try:\n",
    "        # inference in no_grad mode\n",
    "        with torch.no_grad():\n",
    "            corrected = predict(\n",
    "                model, tokenizer, srcs,\n",
    "                encode, decode,\n",
    "                keep_confidence=0.0,\n",
    "                min_error_prob=0.0,\n",
    "                n_iteration=5,\n",
    "                batch_size=2,\n",
    "            )\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error during inference:\", e)\n",
    "        # you could try fallback: smaller batch size or CPU mode here\n",
    "        # e.g. model.cpu(), batch_size=1, etc.\n",
    "\n",
    "    gector_df = pd.DataFrame({\n",
    "        \"Gector_correction\": corrected\n",
    "    })\n",
    "\n",
    "    gector_df.to_excel(f\"ref_gector/{name}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing before and after \n",
    "from diff_match_patch import diff_match_patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ef291",
   "metadata": {},
   "source": [
    "### Gramformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gramformer import Gramformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642331aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_1 = Gramformer(models = 1, use_gpu = False) # 1 = corrector, 2 = detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for srcs, name in zip(sentences, names):\n",
    "    data = []\n",
    "\n",
    "    for sentence in srcs:    \n",
    "        corrected = gf_1.correct(sentence, max_candidates=1)\n",
    "        for c in corrected:\n",
    "            data.append((c))\n",
    "\n",
    "    df_gramformer = pd.DataFrame(data, columns=[\"Gramformer_correction\"])\n",
    "    df_gramformer.to_csv(f\"ref_gramformer/{name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd2690",
   "metadata": {},
   "source": [
    "### Combining the three dfs into one df and save as csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    # Read in\n",
    "    df_gector = pd.read_excel(f\"ref_gector/{name}.xlsx\", index_col = 0)\n",
    "    df_gector.loc[-1] = df_gector.columns \n",
    "    df_gector.index = df_gector.index + 1\n",
    "    df_gector = df_gector.sort_index()\n",
    "    df_gector.columns = [\"message\"]\n",
    "\n",
    "    df_gramformer = pd.read_csv(f\"ref_gramformer/{name}.csv\", index_col = 0)\n",
    "    df_gramformer.loc[-1] = df_gramformer.columns \n",
    "    df_gramformer.index = df_gramformer.index + 1\n",
    "    df_gramformer = df_gramformer.sort_index()\n",
    "    df_gramformer.columns = [\"replacements\"]\n",
    "    \n",
    "    df_language_tool_python = pd.read_excel(f\"ref_language_tool_python/{name}.xlsx\", index_col = 0)\n",
    "\n",
    "    df_gector_gramformer = pd.concat([df_gector, df_gramformer], axis = 1)\n",
    "\n",
    "    # Combine all dataframes into one dataframe\n",
    "    df_total = pd.concat([df_language_tool_python, df_gector_gramformer.reindex(columns = df_language_tool_python.columns)], axis = 0, ignore_index = True)\n",
    "\n",
    "    # Output combined df into out_total folder\n",
    "    df_total.to_csv(f\"ref_results/{name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlplearnerdata-detect-errors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
